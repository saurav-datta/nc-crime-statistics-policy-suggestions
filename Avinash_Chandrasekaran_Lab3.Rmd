---
title: "Lab3 Draft, w203: Statistics for Data Science"
author: "Avinash Chandrasekaran, Deepak Nagaraj, Saurav Datta"
date: "March 31, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, tidy.opts=list(width.cutoff=80), tidy=TRUE)
library(car)
library(corrplot)
library(lmtest)
library(sandwich)
library(stargazer)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(sqldf)
```

# 1. Introduction

Our team has been hired to provide research for a political campaign. The campaign has obtained a dataset of crime statistics for a selection of counties in North Carolina. Our task is to examine the data to help the campaign understand the determinants of crime and to generate policy suggestions that are applicable to local government.

The data provided consists of 25 variables and 91 different observations collected in a given year. Moreover the dataset obtained is a single cross-section of data collected from variety of different sources. For the analysis made in this research, we will assume that the data collected from different counties in NC were randomly sampled.

Our primary analysis of data will include ordinary least squares regressions to make casual estimates and we will clearly explain how omitted variabled may affect our conclusions. We begin our research by conducting exploratory analysis of the dataset to gain a better understanding of the variables.

# 2. Data Input

Let us read the data and have a first look.

```{r}
# Read the csv file
crime_data_raw = read.csv("crime_v2.csv")
```

### Empty rows

```{r include=FALSE}
summary(crime_data_raw)
tail(crime_data_raw, n=8)
```

There appears to be 6 rows of NA's across all variables. We can simply use na.omit(), because the number of all-NA rows matches the count on all the variables.

### Column formatting

We also notice that 'prbconv' is a factor while the rest of the variables are numeric.  West, central and urban are really categorical variables and not integers: we will treat them as such.

```{r}
# Remove NA rows
crime_data = na.omit(crime_data_raw)

# convert factor to numeric for variable prbconv
crime_data$prbconv = as.numeric(levels(crime_data$prbconv)[crime_data$prbconv])
```

### Unused variables

County and Year variables just represent the different counties and the year the data was collected. Year is always 87.  Hence, we can safely remove these from the dataset for further analysis.

```{r}
crime_data = crime_data %>% dplyr::select(-c(year, county))
```

### Duplicate records

We also noticed a duplicate record (record #89) in the dataset. As this could potentially affect our regression analysis, we will remove the duplicate record.

```{r}
duplicated(crime_data)[duplicated(crime_data)==TRUE]
crime_data = distinct(crime_data)
```

# 3. Exploratory Data Analysis

We will start with an explanatory note on transformations.  Any skew in the original data may cause the residuals not to follow normal distribution.  If this happens, it violates an assumption of the LS regression model: we will not be able to draw inferences from our model.  Hence it is important to ensure our residuals to follow normal distribution as much as possible, and to transform our predictors if that helps.

We will now try to get a sense of each variable in the dataset.

## Single variable analysis

```{r include=FALSE}
structure(crime_data)
```

There are a total of 90 observations across 23 different variables. We will now explore each of the variables collected in the data.

```{r util_fns}
# Utility function to describe a column variable
f_describe_col = function (col, do_log=FALSE, plot_model=FALSE, do_sqrt=FALSE) {
  y = log(crime_data$crmrte)
  print(summary(col))
  par(mfrow=c(2,2))
  if (is.numeric(col)) {
    hist(col, main="Histogram")
    boxplot(col, main="Box plot")
  }
  if (do_log == TRUE) {
    x = log(col)
    hist(x, main="Histogram, log")
  } else if (do_sqrt == TRUE) {
    x = sqrt(col)
    hist(x, main="Histogram, sqrt")
  } else {
    x = col
  }
  if (is.numeric(col)) {
    print(signif(cor(x, y), 3))
  }
  m = lm(y ~ x)
  print(summary(m))
  plot(x, y, main="Cor. with crime rate")
  if (is.numeric(col))
    abline(m, col="blue")
  if (plot_model == TRUE) {
    plot(m)
    if (do_log == TRUE) {
      m = lm(y ~ x)
      plot(m)
    }
  }
}
```

### Population density

We will start with population density.

```{r}
f_describe_col(crime_data$density, do_log=TRUE, plot_model=TRUE)
```

We see that density is a highly skewed distribution.  Most observations are from counties with low population density.  However, observation #79 has Cook's distance beyond 1, meaning extreme leverage:

```{r}
crime_data %>% slice(79) %>% select(everything())
```

The density is 2E-5, which is extremely low.  With so few people, the observation may not add a lot of meaning.  It also affects the model.  We will remove this observation from the dataset and replot.

```{r}
crime_data = crime_data %>% slice(-79)
f_describe_col(crime_data$density, do_log=TRUE, plot_model=TRUE)
```

The histogram of density shows quite a bit positive skew. The log transformation shows a more promising normal distribution.  There are no more outliers with large leverage as measured by Cook's distance.

We see high positive correlation with crime rate.  We will surely consider this variable in our model.

### Police per capita

```{r}
f_describe_col(crime_data$polpc, do_log=TRUE, plot_model=TRUE)
```

Police per capita has positive skew.  Taking log helps, but we still see a very large outlier.  Fitting a model, we see that observation #51 has Cook's distance beyond 1.  This is a lot of leverage.  Let us look at the observation:

```{r}
crime_data %>% slice(51) %>% select(everything())
```

The crime rate, at 0.005, is extremely low for police per capita close to the maximum.  It is questionable whether this observation is accurate.  We will remove this reading from the dataset and replot.

```{r}
crime_data = crime_data %>% slice(-51)
f_describe_col(crime_data$polpc, do_log=TRUE, plot_model=TRUE)
crime_data$log_polpc = log(crime_data$polpc)
```

The distribution looks better now.  We see quite strong positive correlation of 0.6 with crime rate: high number of police per capita is associated with high crime rate.  It is probably a cause, rather than a result.  More police may have been deployed to deal with higher amount of crime.

For our first model, we will *not* include this variable.

### Crime rate

```{r}
summary(crime_data$crmrte)
par(mfrow=c(2,2))
hist(crime_data$crmrte, main = "Crime rate", 
     xlab="Crime Rate")
boxplot(crime_data$crmrte, main = "Boxplot")
hist(log(crime_data$crmrte), main = "Crime rate", 
     xlab="Log of Crime Rate")
```

The crime rate variable is the key dependent variable of interest. Looking at the histogram, the distribution is positively skewed to the left. We can take the log transformation which makes the variable appear more normally distributed.

### Probability of arrest

```{r}
f_describe_col(crime_data$prbarr, plot_model=TRUE)
```

The plot looks fairly normal; there is only one outlier.

There is fairly negative correlation of -0.37: as probability of arrests increases, crime rate goes down.  It may be that arrests are a deterrent.

We will include $prbarr$ in our model.

### Probability of conviction

```{r}
f_describe_col(crime_data$prbconv, do_log=TRUE)
crime_data$log_prbconv = log(crime_data$prbconv)
```

This variable has quite a bit of left skew.  It also has many outliers after the 3rd quartile.  There are a few beyond 1 as well.  Again, this is because we are not looking at a real probability but a ratio of convictions to arrests.  It is possible, although perhaps uncommon, that a suspect is arrested once but convicted on multiple charges.

Taking a log transform improves the skew, although the spread is still quite a bit.  There are no outliers with large influence as measured by Cook's distance.

There is moderate negative correlation with crime rate of -0.3.  As convictions go up, crime rate goes down.  Since we have already considered $prbarr$, let us check if $prbconv$ has high correlation with $prbarr$:

```{r}
print(cor(crime_data$prbarr, crime_data$prbconv))
print(cor(crime_data$prbarr, crime_data$log_prbconv))
```

Not much.  We will include $log_prbconv$ in our model.

### Probability of prison sentence

```{r}
f_describe_col(crime_data$prbpris)
```

This histogram plot looks fairly normal and we don't observe any outliers.  However, correlation is almost nonexistent wrt crime rate.

We will *not* consider this variable in our model.

### Average sentence duration

```{r}
f_describe_col(crime_data$avgsen, do_log=TRUE)
```

The average sentence in days looks slightly positive skewed, which we can correct with a log transform.  But correlation is absent with respect to crime rate.  It is interesting because we would expect that longer sentences would deter crime.

Perhaps we can use this data to make a policy recommendation to reduce sentences over long periods of time, or to be more lenient in pardoning criminals already serving long sentences.

We will *not* consider this variable in our model.

### Tax revenue per capita

```{r}
f_describe_col(crime_data$taxpc, do_log=TRUE, plot_model=TRUE)
```

Tax revenue also shows positive skew, with one outlier indicating high tax revenue per capita (>100).  It does not show a lot of leverage, however, so we will keep the value.

We also see considerable positive correlation with crime rate.  It may be that tax revenue is a proxy for wealth, and high amount of wealth attracts crime.

We will *not* include this variable in a first model.

### Urban population

```{r}
print(length(crime_data$urban[crime_data$urban == 1]))
urban_crime_data = crime_data %>% filter(urban == 1) %>% dplyr::select(-urban)
rural_crime_data = crime_data %>% filter(urban == 0) %>% dplyr::select(-urban)
par(mfrow=c(1,2))
lmts = range(urban_crime_data$crmrte, rural_crime_data$crmrte)
boxplot(urban_crime_data$crmrte, main="Crime: Urban", ylim=lmts)
boxplot(rural_crime_data$crmrte, main="Crime: Rural", ylim=lmts)
```

It is worth noting that there are only 8 observations classified urban in this dataset.  Median crime rate in urban regions is double that of rural regions.

Let us fit a model and see if our variable is salient.

```{r}
print(cor(crime_data$urban, log(crime_data$crmrte)))
m = lm(log(crmrte) ~ factor(urban), data=crime_data)
print(summary(m))
```

We do see a strong correlation between observations classified "urban" and crime rate, and the same is reflected by the low p-value in the model summary.

Let us check if there is correlation between "urban" and "density":

```{r}
cor(crime_data$density, crime_data$urban)
```

This is quite high, so we run a risk of multicollinearity.

Therefore, and since we have already selected density (with an additional advantage of more number of observations), we will *not* include this variable in our model.

### Percent minority

```{r}
f_describe_col(crime_data$pctmin80, plot_model=TRUE, do_sqrt=TRUE)
crime_data$sqrt_pctmin80 = sqrt(crime_data$pctmin80)
```

Minority percentage has positive skew, but no outliers.  Taking square root reshapes the distribution nicely.

There is a fair amount of positive correlation with crime rate (0.27).  It may be that as minorities increase, there is loss of social homogeneity.

We will include this (transformed) variable in our model.

### Wage distribution

```{r}
par(mfrow=(c(3,3)))
hist(crime_data$wcon, breaks=20,
     main="Hist of wcon",ylab="Frequency", xlab="wagecon")
hist(crime_data$wtuc, breaks=20,
     main="Hist of wtuc",xlab="wagetuc", ylab="")
hist(crime_data$wloc, breaks=20,
     main="Hist of wloc",xlab="wageloc", ylab="")
hist(crime_data$wtrd, breaks=20,
     main="Hist of wtrd",ylab="Frequency", xlab="wagetrd")
hist(crime_data$wfir, breaks=20,
     main="Hist of wfir",xlab="wagefir", ylab="")
hist(crime_data$wser, breaks=20,
     main="Hist of wser",xlab="wageser", ylab="")
hist(crime_data$wmfg, breaks=20,
     main="Hist of wmfg",ylab="Frequency", xlab="wagemfg")
hist(crime_data$wfed, breaks=20,
     main="Hist of wfed",xlab="wagefed", ylab="")
hist(crime_data$wsta, breaks=20,
     main="Hist of wsta",xlab="wagesta", ylab="")

```
Most of the wage variables conform to normal distributions.
```{r}
summary(crime_data$wser)
crime_data= filter(crime_data, wser<2177)
```
Wage in service industry does seem to have one strange outlier that is causing some skewness in the plot.
We believe this is likely an error and are thus removing it from consideration.


### Offense Mix & Percent of young males
```{r}
par(mfrow=c(1,2))
hist(crime_data$mix, main="Histogram of offense mix",
     xlab="offense mix, f2f/other")
hist(log(crime_data$mix), main="Histogram of offense mix",
     xlab="Log of offense mix")
```
Log transformation of offense mix is more normal while the percent of young males has a heavy left positive skew regardless of the log transformation

```{r}
par(mfrow=c(1,2))
hist(crime_data$pctymle, main="Hist of percent of young males",
     xlab="pct young males")
hist(log(crime_data$pctymle), main="Histogram of percent of young males",
     xlab="Log of pct young males")
```

## Data Transformation
Based on the univariate analysis performed above, we can opt to take the following transformations of the variables to make better analysis and judgement calls:

Log transformation of the Crime Rate, Police per Capita, Density per sq. mile, Tax revenue per capita
And finally scaling the percent (percent young male) and probabilities (arrest, conviction and prison sentence) to be between 0-100
```{r}
crime_data$log_crmrte = log(crime_data$crmrte)
crime_data$log_density = log(crime_data$density)
crime_data$log_polpc = log(crime_data$polpc)
crime_data$log_taxpc = log(crime_data$taxpc)
crime_data$adj_pctymle = crime_data$pctymle *100
crime_data$adj_prbarr = crime_data$prbarr *100
crime_data$adj_prbconv = crime_data$prbconv *100
crime_data$adj_prbpris = crime_data$prbpris *100

```

A final sumary table of our dataset with all transformation and data cleansing performed is displayed below:
```{r mylatextable, results = "asis"}
stargazer(crime_data, title = "Descriptive Statistics", digits=1)
```

## Bi-variate Analysis

The correlation plot between the different variables is as follows:
```{r}
corrplot(cor(crime_data[,
                        c("log_crmrte", "adj_prbarr", "adj_prbconv", "adj_prbpris", "avgsen", 
                          "log_polpc", "log_density", "log_taxpc", "pctmin80", "mix", 
                          "adj_pctymle")]), type = "upper")
corrplot(cor(crime_data[,
                        c("log_crmrte", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", 
                          "wsta", "wloc")]), type = "upper")
```
We can see that there is a high positive correlation between:

- log of crime rate vs. log of policy per capita, log of tax revenue per capita, log of density and percent young male

- log of crime rate vs. most of the wage variables


And there is a high negative correlation between:

- log of crime rate vs. probability of arrests and conviction


The positive correlation observed makes sense for the following reasons:

1) More densely populated regions tends to observe more crimes
2) More wealthy areas (more wages and taxes) tend to have more crimes
3) More crimes leads to more police presence in a particular county to monitor and reduce crime rate

The negative correlations can be further observed using:
```{r}
par(mfrow=c(1,2))
plot(crime_data$adj_prbarr, crime_data$log_crmrte,
     main="Probability of arrest", ylab="Log Crime rate", xlab="Prob on arrest")
abline(lm(crime_data$log_crmrte ~ crime_data$adj_prbarr))
plot(crime_data$adj_prbconv, crime_data$log_crmrte,
     main="Probability of conviction vs. crime rate", ylab="Log Crime rate", 
     xlab="Prob on conv")
abline(lm(crime_data$log_crmrte ~ crime_data$adj_prbconv))
```
As seen above, as the probability of arrests and conviction go down, there are more criminals on the loose which leads to higher crime rates observed

### TODO: Talk about other possible correlations here?

### TODO: Discuss other interesting bi-variate analysis?

# 3. Model Specification and Assumptions

In our earlier analysis, we observed some key relationships between crime rate and other variables presented. Some of these variables had high positive correlation to crime rate while some others exhibited strong negative correlation.

For our first simple model, we will choose a subset of these variables that we believe are most important determinants of crime rate.

## Model 1
$$ log(Crime Rate) = \beta_0 + \beta_1 log(Density) + \beta_2(YoungMale) + \beta_3(Minority) + u$$
It is common knowledge that areas with higher density have more crime. Therefore we include that factor in our model.
Similarly we hypothesized that crime rate is high among minority and young male population, so we round off our model with
that factored in as well.

```{r}
model1 = lm(log(crmrte) ~ (log_density)+pctymle+pctmin80, data=crime_data)
model1$coefficients
par(mfrow=c(2,2))
plot(model1)
AIC(model1)
summary(model1)$r.squared
summary(model1)$adj.r.squared
```

## Model 2
high probability of arrests and conviction act as deterrents to crime.

$$ log(Crime Rate) = \beta_0 + \beta_1 log(Density) + \beta_2 (YoungMale) + \beta_3(Minority) + \beta_4(Conviction) + \beta_5 (Arrest) + \beta_6 (Tax) + u$$
```{r}
model2 = lm(log(crmrte) ~ (log_density)+pctymle+pctmin80+adj_prbarr+
            adj_prbconv+taxpc, data=crime_data)
model2$coefficients
par(mfrow=c(2,2))
plot(model2)
AIC(model2)
summary(model2)$r.squared
```

## Model 3
everything

```{r}
model3 = lm(log(crmrte) ~ (log_density)+pctymle+pctmin80+adj_prbarr+adj_prbconv+
              taxpc+log_polpc, data=crime_data)
model3$coefficients
par(mfrow=c(2,2))
plot(model3)
AIC(model3)
summary(model3)$r.squared
```

```{r results = "asis"}
stargazer(model1, model2, model3)
```
# 5. Discussion of omitted variables (Identify what you think are the 5-10 most important omitted variables that bias results you care about.)

Education

Unemployment

Poverty


```{r}

```