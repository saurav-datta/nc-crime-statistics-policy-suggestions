---
title: "Lab3 Draft, w203: Statistics for Data Science"
author: "Avinash Chandrasekaran, Deepak Nagaraj, Saurav Datta"
date: "March 31, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, tidy.opts=list(width.cutoff=80), tidy=TRUE)
library(car)
library(corrplot)
library(lmtest)
library(sandwich)
library(stargazer)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(sqldf)
library(knitr)
library(kableExtra)
```

# 1. Introduction

Our team has been hired to provide research for a political campaign. The campaign has obtained a dataset of crime statistics for a selection of counties in North Carolina. Our task is to examine the data to help the campaign understand the determinants of crime and to generate policy suggestions that are applicable to local government.

The data provided consists of 25 variables and 91 different observations collected in a given year. Moreover the dataset obtained is a single cross-section of data collected from variety of different sources. For the analysis made in this research, we will assume that the data collected from different counties in NC were randomly sampled.

Our primary analysis of data will include ordinary least squares regressions to make casual estimates and we will clearly explain how omitted variabled may affect our conclusions. We begin our research by conducting exploratory analysis of the dataset to gain a better understanding of the variables.

# 2. Data Input

Let us read the data and have a first look.

```{r}
# Read the csv file
crime_data_raw = read.csv("crime_v2.csv")
```

### Empty rows

```{r include=FALSE}
summary(crime_data_raw)
tail(crime_data_raw, n=8)
```

There appears to be 6 rows of NA's across all variables. We can simply use na.omit(), because the number of all-NA rows matches the count on all the variables.

```{r}
# Remove NA rows
crime_data = na.omit(crime_data_raw)
```

### Column formatting

We also notice that 'prbconv' is a factor while the rest of the variables are numeric.

```{r}
# convert factor to numeric for variable prbconv
crime_data$prbconv = as.numeric(levels(crime_data$prbconv)[crime_data$prbconv])
```

### Unused variables

County and Year variables just represent the different counties and the year the data was collected. Year is always 87.  Hence, we can safely remove these from the dataset for further analysis.

```{r}
crime_data = crime_data %>% dplyr::select(-c(year, county))
```

### Duplicate records

We also noticed a duplicate record (record #89) in the dataset. As this could potentially affect our regression analysis, we will remove the duplicate record.

```{r}
duplicated(crime_data)[duplicated(crime_data)==TRUE]
crime_data = distinct(crime_data)
```

# 3. Influential outliers

This section was filled in after a first pass at the variables.  This is so that we remove any malformed observations in the beginning, and also to show the removed observations.

There is a large outlier for wages in service industry, $wser$.  Observation #84 has very large influence as shown by Cook's distance.

```{r}
m = lm(log(crime_data$crmrte) ~ crime_data$wser)
plot(m, which=5)
```

Next, let us consider $density$.  Observation #79 has Cook's distance beyond 1, meaning extreme leverage:

```{r}
m = lm(log(crime_data$crmrte) ~ log(crime_data$density))
plot(m, which=5)
```

Finally, we consider $polpc$.  Police per capita has positive skew.  Taking log helps, but we still see a very large outlier.  Fitting a model, we see that observation #51 has Cook's distance beyond 1.  This is a lot of leverage:

```{r}
m = lm(log(crime_data$crmrte) ~ log(crime_data$polpc))
plot(m, which=5)
```

Here are all the outlier observations:

```{r}
outlier_data = crime_data %>% slice(c(51, 79, 84))
structure(outlier_data)
```

The first observation above has very low crime rate at very high police per capita.  The second has extremely low density.  The third has extremely high wages in the service industry.

The observations are questionable and affect our model because of their high influence, as measured by Cook's distance.  We will remove them.

```{r}
crime_data <- crime_data %>% slice(-c(51, 79, 84))
```

# 4. Exploratory Data Analysis

```{r util_fns}
# Utility function to describe a column variable
f_describe_col = function (col, do_log=FALSE, plot_model=FALSE, do_sqrt=FALSE) {
  y = log(crime_data$crmrte)
  par(mfrow=c(2,2))
  if (is.numeric(col)) {
    hist(col, main="Histogram")
    boxplot(col, main="Box plot")
  }
  if (do_log == TRUE) {
    x = log(col)
    hist(x, main="Histogram, log")
  } else if (do_sqrt == TRUE) {
    x = sqrt(col)
    hist(x, main="Histogram, sqrt")
  } else {
    x = col
  }
  if (is.numeric(col)) {
    print(paste("Correlation: ", signif(cor(x, y), 3)))
  }
  m = lm(y ~ x)
  plot(x, y, main="Cor. with crime rate")
  if (is.numeric(col))
    abline(m, col="blue")
  if (plot_model == TRUE) {
    if (do_log == TRUE) {
      m = lm(y ~ x)
    }
    plot(m, which=5)
  }
}
```

We will start with an explanatory note on transformations.  Any skew in the original data may cause the residuals not to follow normal distribution.  If this happens, it violates an assumption of the LS regression model: we will not be able to draw inferences from our model.  Hence it is important to ensure our residuals to follow normal distribution as much as possible, and to transform our predictors if that helps.

We will now try to get a sense of each variable in the dataset.

## Single variable analysis

There are a total of 90 observations across 23 different variables. We will now explore each of the variables collected in the data.

### Crime rate

Crime rate is the key dependent variable of interest. 

```{r}
summary(crime_data$crmrte)
par(mfrow=c(2,2))
hist(crime_data$crmrte, main = "Crime rate", 
     xlab="Crime Rate")
boxplot(crime_data$crmrte, main = "Boxplot")
hist(log(crime_data$crmrte), main = "Crime rate", 
     xlab="Log of Crime Rate")
```

Looking at the histogram, the distribution is positively skewed to the left. We can take the log transformation which makes the variable appear more normally distributed.

### Probability of arrest

```{r}
f_describe_col(crime_data$prbarr, plot_model=TRUE)
```

The plot looks fairly normal; there is only one outlier.

There is fairly negative correlation of -0.37: as probability of arrests increases, crime rate goes down.  It may be that arrests are a deterrent, indicating causality.

We will include $prbarr$ in our model.

### Probability of conviction

```{r}
f_describe_col(crime_data$prbconv, do_log=TRUE)
crime_data$log_prbconv = log(crime_data$prbconv)
```

This variable has quite a bit of left skew.  It also has many outliers after the 3rd quartile.  There are a few beyond 1 as well.  Again, this is because we are not looking at a real probability but a ratio of convictions to arrests.  It is possible, although perhaps uncommon, that a suspect is arrested once but convicted on multiple charges.

Taking a log transform improves the skew, although the spread is still quite a bit.  There are no outliers with large influence as measured by Cook's distance.

There is moderate negative correlation with crime rate of -0.3.  As convictions go up, crime rate goes down.  Since we have already considered $prbarr$, let us check if $prbconv$ has high correlation with $prbarr$:

```{r}
print(cor(crime_data$prbarr, crime_data$prbconv))
print(cor(crime_data$prbarr, crime_data$log_prbconv))
```

Not much.  We will include *log_prbconv* in our model.

### Probability of prison sentence

```{r}
f_describe_col(crime_data$prbpris)
```

This histogram plot looks fairly normal and we don't observe any outliers.  However, correlation is almost nonexistent wrt crime rate.  This is interesting, because whether a crime results in spending time in prison does not seem to affect crime.  This can shape government policies on whether to send criminals to prison or to find alternative ways to reform them.

We will *not* consider this variable in our model.

### Average sentence duration

```{r}
f_describe_col(crime_data$avgsen, do_log=TRUE)
```

The average sentence in days looks slightly positive skewed, which we can correct with a log transform.  But correlation is absent with respect to crime rate.  It is interesting because we would expect that longer sentences would deter crime.

Perhaps we can use this data to make a policy recommendation to reduce sentences over long periods of time, or to be more lenient in pardoning criminals already serving long sentences.

We will *not* consider this variable in our model.

### Police per capita

Note: In our first pass, we found an influential outlier with very low crime rate, even at very high police per capita.  We removed it, as mentioned in the section on outliers.

Police per capita has positive skew.  Taking log helps:

```{r}
f_describe_col(crime_data$polpc, do_log=TRUE, plot_model=TRUE)
crime_data$log_polpc = log(crime_data$polpc)
```

The distribution looks better now.  We see fairly strong positive correlation of 0.6 with crime rate: high number of police per capita is associated with high crime rate.  It is probably a cause, rather than a result.  More police may have been deployed to deal with higher amount of crime.  If that is the case, it is worth questioning further why the additional police has not lowered the crime rate: are they ineffective?

For our first model, we will *not* include this variable.

### Population density

Note: In our first pass, we found an influential outlier with very low density and removed it, as mentioned in the section on outliers.

```{r}
f_describe_col(crime_data$density, do_log=TRUE, plot_model=TRUE)
```

The histogram of density shows quite a bit positive skew. The log transformation shows a more promising normal distribution.  There are no outliers with large leverage as measured by Cook's distance.

We see high positive correlation with crime rate.  It may be that high population density indicates greater scope for hiding or cooperation in order to commit crime, indicating causality.  We will surely consider this variable in our model.

### Tax revenue per capita

```{r}
f_describe_col(crime_data$taxpc, do_log=TRUE, plot_model=TRUE)
```

Tax revenue also shows positive skew, with one outlier indicating high tax revenue per capita (>100).  It does not show a lot of leverage, however, so we will keep the value.

We also see considerable positive correlation with crime rate.  It may be that tax revenue is a proxy for wealth, and high amount of wealth attracts crime.  On the other hand, it is worth checking if we are spending tax dollars wisely in combating crime: if that were the case, counties with higher tax revenue would probably see lower crime.

We will *not* include this variable in a first model.

### Urban population

```{r}
print(length(crime_data$urban[crime_data$urban == 1]))
urban_crime_data = crime_data %>% filter(urban == 1) %>% dplyr::select(-urban)
rural_crime_data = crime_data %>% filter(urban == 0) %>% dplyr::select(-urban)
par(mfrow=c(1,2))
lmts = range(urban_crime_data$crmrte, rural_crime_data$crmrte)
boxplot(urban_crime_data$crmrte, main="Crime: Urban", ylim=lmts)
boxplot(rural_crime_data$crmrte, main="Crime: Rural", ylim=lmts)
```

It is worth noting that there are only 8 observations classified urban in this dataset.  Median crime rate in urban regions is double that of rural regions.

Let us fit a model and see if our variable is salient.

```{r}
print(cor(crime_data$urban, log(crime_data$crmrte)))
m = lm(log(crmrte) ~ factor(urban), data=crime_data)
print(summary(m))
```

We do see a strong correlation between observations classified "urban" and crime rate, and the same is reflected by the low p-value in the model summary.

Let us check if there is correlation between "urban" and "density":

```{r}
cor(crime_data$density, crime_data$urban)
```

This is quite high, so we run a risk of multicollinearity.

Therefore, and since we have already selected density (with an additional advantage of more number of observations), we will *not* include this variable in our model.

### Percent minority

```{r}
f_describe_col(crime_data$pctmin80, plot_model=TRUE, do_sqrt=TRUE)
crime_data$sqrt_pctmin80 = sqrt(crime_data$pctmin80)
```

Minority percentage has positive skew, but no outliers.  Taking square root reshapes the distribution nicely.

There is a fair amount of positive correlation with crime rate (0.27).  It may be that as minorities increase, there is loss of social homogeneity and/or hate crime.

We will include this (transformed) variable in our model.

### Wage distribution

Note: In our first pass, we found an influential outlier in services wages and removed it, as mentioned in the section on outliers.

```{r}
par(mfrow=c(3,3))
hist(crime_data$wcon)
hist(crime_data$wloc)
hist(crime_data$wtrd)
hist(crime_data$wtuc)
hist(crime_data$wfir)
hist(crime_data$wser)
hist(crime_data$wmfg)
hist(crime_data$wfed)
hist(crime_data$wsta)
```

Most of the wage variables conform to normal distributions.  We do not have to worry about transformations.

Let us look which of them have high correlation with crime rate, considering all those with $R > 0.25$ (arbitrarily).

```{r}
wage_cols = c("wcon", "wloc", "wtrd", "wtuc", "wfir", 
              "wser", "wmfg", "wfed", "wsta")
cor(log(crime_data$crmrte), crime_data[, wage_cols])
```

This eliminates wsta and wtuc, but we are still left with 7 categories.

* wfed (0.50)
* wcon (0.37)
* wtrd (0.37)
* wser (0.34)
* wloc (0.28)
* wmfg (0.28)
* wfir (0.27)

As a different approach, let us check if the wages have high correlation among them.  This will allow us to eliminate possible multi-collinearity.

```{r}
corrplot(cor(crime_data[, wage_cols]), type="upper", diag=TRUE, addCoef.col="white", addCoefasPercent = TRUE, order="hclust", method="ellipse")
```

Indeed, a lot of the wage categories above have a high degree of correlation among them, but all are less than 70.  We cannot eliminate any wage categories this way.

As a third approach, let us check for variance inflation instead:

```{r}
m = lm(log(crmrte) ~ wfed + wcon + wtrd + wser + wloc + wmfg + wfir + wsta + wtuc, data=crime_data)
print(vif(m))
```

Again, no VIF is above 5.  This procedure also does not eliminate any wage categories.

We can take a call, and choose $wcon$ as a proxy for a first model.  We can include other wages in a second model.

```{r}
f_describe_col(crime_data$wcon, plot_model=TRUE)
```

### Offense Mix

```{r}
f_describe_col(crime_data$mix, do_log=TRUE, plot_model=TRUE)
```

Offense mix does not seem to have any correlation with crime rate.  The distribution is skewed, but a log transform fixes it.  Outliers exist, but none have leverage as detected by Cook's distance.

We will *not* include offense mix in our models.

### Percent of young males

```{r}
f_describe_col(crime_data$pctymle, do_sqrt=TRUE, plot_model=TRUE)
crime_data$sqrt_pctymle = sqrt(crime_data$pctymle)
```

We see moderate positive correlation with higher percentage of young males.  There is positive skew, which we correct by taking a square root.  Boxplot shows outliers, but none has outsized influence (Cook's distance < 0.5).

A high percentage of young males can indicate higher aggressiveness and risk, causing higher rate of crime.  We will include this variable in our model.

### Categorical variables

We have the following categorical variables in the dataset:

* Direction: west, central, other
* Urban or rural

We will use these to come up with separate models, based on different factors, later in this analysis. [TODO: Saurav]

# Summary of variables

Here is a summary table of variables used in our models.

| Variable | Transform? | Model1? | Model2? | Model3? | Remarks |
|----------|------------|---------|---------|---------|---------|
| county   | N/A        |         |         |         | Unused  |
| year     | N/A        |         |         |         | Unused  |
| prbarr   |            | Y       | Y       | Y       |         |
| prbconv  | log        | Y       | Y       | Y       |         |
| prbpris  |            |         |         | Y       | No corr. found |
| avgsen   |            |         |         | Y       | No corr. found |
| polpc    | log        |         | Y       | Y       | Effect, not cause |
| density  | log        | Y       | Y       | Y       | Causal  |
| taxpc    | log        |         | Y       | Y       | Omit var: wealth |
| west     | N/A        |         |         |         | Categ, sep. model |
| central  | N/A        |         |         |         | Categ, sep. model |
| urban    |            |         |         |         | Cor. with density |
| pctmin80 | sqrt       | Y       | Y       | Y       | Causal  |
| wcon     |            | Y       | Y       | Y       | Proxy for wages |
| wtuc     |            |         |         | Y       | Low corr. found |
| wtrd     |            |         | Y       | Y       |         |
| wfir     |            |         | Y       | Y       |         | 
| wser     |            |         | Y       | Y       |         |
| wmfg     |            |         | Y       | Y       |         |
| wfed     |            |         | Y       | Y       |         |
| wsta     |            |         |         | Y       | Low corr. found |
| wloc     |            |         | Y       | Y       |         |
| mix      | log        |         |         | Y       | No corr. found |
| pctymle  | sqrt       | Y       | Y       | Y       | Causal  |

## Data Transformation
Based on the univariate analysis performed above, we can opt to take the following transformations of the variables to make better analysis and judgement calls:

Log transformation of the Crime Rate, Police per Capita, Density per sq. mile, Tax revenue per capita
And finally scaling the percent (percent young male) and probabilities (arrest, conviction and prison sentence) to be between 0-100
```{r}
crime_data$log_crmrte = log(crime_data$crmrte)
crime_data$log_density = log(crime_data$density)
crime_data$log_polpc = log(crime_data$polpc)
crime_data$log_taxpc = log(crime_data$taxpc)
crime_data$adj_pctymle = crime_data$pctymle *100
crime_data$adj_prbarr = crime_data$prbarr *100
crime_data$adj_prbconv = crime_data$prbconv *100
crime_data$adj_prbpris = crime_data$prbpris *100

```

A final sumary table of our dataset with all transformation and data cleansing performed is displayed below:
```{r mylatextable, results = "asis"}
stargazer(crime_data, title = "Descriptive Statistics", digits=1)
```

## Bi-variate Analysis

The correlation plot between the different variables is as follows:
```{r}
corrplot(cor(crime_data[,
                        c("log_crmrte", "adj_prbarr", "adj_prbconv", "adj_prbpris", "avgsen", 
                          "log_polpc", "log_density", "log_taxpc", "pctmin80", "mix", 
                          "adj_pctymle")]), type = "upper")
corrplot(cor(crime_data[,
                        c("log_crmrte", "wcon", "wtuc", "wtrd", "wfir", "wser", "wmfg", "wfed", 
                          "wsta", "wloc")]), type = "upper")
```
We can see that there is a high positive correlation between:

- log of crime rate vs. log of policy per capita, log of tax revenue per capita, log of density and percent young male

- log of crime rate vs. most of the wage variables


And there is a high negative correlation between:

- log of crime rate vs. probability of arrests and conviction


The positive correlation observed makes sense for the following reasons:

1) More densely populated regions tends to observe more crimes
2) More wealthy areas (more wages and taxes) tend to have more crimes
3) More crimes leads to more police presence in a particular county to monitor and reduce crime rate

The negative correlations can be further observed using:
```{r}
par(mfrow=c(1,2))
plot(crime_data$adj_prbarr, crime_data$log_crmrte,
     main="Probability of arrest", ylab="Log Crime rate", xlab="Prob on arrest")
abline(lm(crime_data$log_crmrte ~ crime_data$adj_prbarr))
plot(crime_data$adj_prbconv, crime_data$log_crmrte,
     main="Probability of conviction vs. crime rate", ylab="Log Crime rate", 
     xlab="Prob on conv")
abline(lm(crime_data$log_crmrte ~ crime_data$adj_prbconv))
```
As seen above, as the probability of arrests and conviction go down, there are more criminals on the loose which leads to higher crime rates observed

### TODO: Talk about other possible correlations here?

### TODO: Discuss other interesting bi-variate analysis?

# 3. Model Specification and Assumptions

In our earlier analysis, we observed some key relationships between crime rate and other variables presented. Some of these variables had high positive correlation to crime rate while some others exhibited strong negative correlation.

For our first simple model, we will choose a subset of these variables that we believe are most important determinants of crime rate.

## Model 1
$$ log(Crime Rate) = \beta_0 + \beta_1 log(Density) + \beta_2(YoungMale) + \beta_3(Minority) + u$$
It is common knowledge that areas with higher density have more crime. Therefore we include that factor in our model.
Similarly we hypothesized that crime rate is high among minority and young male population, so we round off our model with
that factored in as well.

```{r}
model1 = lm(log(crmrte) ~ (log_density)+pctymle+pctmin80, data=crime_data)
model1$coefficients
par(mfrow=c(2,2))
plot(model1)
AIC(model1)
summary(model1)$r.squared
summary(model1)$adj.r.squared
```

## Model 2
high probability of arrests and conviction act as deterrents to crime.

$$ log(Crime Rate) = \beta_0 + \beta_1 log(Density) + \beta_2 (YoungMale) + \beta_3(Minority) + \beta_4(Conviction) + \beta_5 (Arrest) + \beta_6 (Tax) + u$$
```{r}
model2 = lm(log(crmrte) ~ (log_density)+pctymle+pctmin80+adj_prbarr+
            adj_prbconv+taxpc, data=crime_data)
model2$coefficients
par(mfrow=c(2,2))
plot(model2)
AIC(model2)
summary(model2)$r.squared
```

## Model 3
everything

```{r}
model3 = lm(log(crmrte) ~ (log_density)+pctymle+pctmin80+adj_prbarr+adj_prbconv+
              taxpc+log_polpc, data=crime_data)
model3$coefficients
par(mfrow=c(2,2))
plot(model3)
AIC(model3)
summary(model3)$r.squared
```

```{r results = "asis"}
stargazer(model1, model2, model3)
```
# 5. Discussion of omitted variables (Identify what you think are the 5-10 most important omitted variables that bias results you care about.)

Education

Unemployment

Poverty


```{r}

```